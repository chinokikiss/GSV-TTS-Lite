import re
import os
import time
import json
import uuid
import torch
import logging
import argparse
import gradio as gr
import numpy as np
from datetime import datetime
from pedalboard import Pedalboard, Compressor, HighpassFilter, PeakFilter, Reverb, Gain
import pyloudnorm as pyln

from gsv_tts import TTS, AudioClip

logging.getLogger('asyncio').setLevel(logging.CRITICAL)
logging.getLogger('httpx').setLevel(logging.CRITICAL)


# Copied from https://github.com/Icelinea/BetterAIVoice/blob/main/process.py
def enhance_audio(audio_data, sample_rate):
    # 1. æ„å»ºç¾åŒ–é“¾
    board = Pedalboard([
        # å»é™¤ä½é¢‘æµ‘æµŠ
        HighpassFilter(cutoff_frequency_hz=80),
        
        # å¢åŠ å¥³å£°ç£æ€§ï¼š250Hz-350Hz æå‡
        PeakFilter(cutoff_frequency_hz=300, gain_db=2.5, q=1.0),
        
        # å‹åˆ¶ AI é½¿éŸ³ï¼š6kHz-8kHz å¾®å¾®å‰Šå‡
        PeakFilter(cutoff_frequency_hz=7000, gain_db=-3.0, q=2.0),
        
        # ç¨³å®šåŠ¨æ€ï¼šé˜²æ­¢æœ‰å£°ä¹¦éŸ³é‡å¿½å¤§å¿½å°
        Compressor(threshold_db=-18, ratio=3.5),
        
        # èµ‹äºˆå½•éŸ³æ£šç©ºé—´æ„Ÿ
        # ä½¿ç”¨å†…å»º Reverb æ¨¡æ‹Ÿ Ambience é¢„è®¾ (Mix 3%, æå°è¡°å‡)
        Reverb(room_size=0.1, dry_level=0.97, wet_level=0.03, damping=0.5),
        
        # æœ€ç»ˆå¢ç›Šè¡¥å¿
        Gain(gain_db=2)
    ])

    # 2. æ‰§è¡Œå¤„ç†
    effected = board(audio_data, sample_rate)
    input_for_norm = effected.reshape(-1, 1)

    # 3. å“åº¦æ ‡å‡†åŒ–
    # æµ‹é‡å½“å‰å“åº¦
    meter = pyln.Meter(sample_rate) 
    loudness = meter.integrated_loudness(input_for_norm)
    # å°†å“åº¦ç»Ÿä¸€è°ƒæ•´è‡³ -18.0 LUFS (æ’­å®¢æ ‡å‡†)
    normalized_audio = pyln.normalize.loudness(input_for_norm, loudness, -18.0).T

    return normalized_audio.flatten()


def upload_gpt(new_gpt):
    if not new_gpt is None:
        for gpt in tts.get_gpt_list():
            tts.unload_gpt_model(gpt)
        
        tts.load_gpt_model(new_gpt.strip('"â€œâ€'))

def upload_sovits(new_sovits):
    if not new_sovits is None:
        for sovits in tts.get_sovits_list():
            tts.unload_sovits_model(sovits)
        
        tts.load_sovits_model(new_sovits.strip('"â€œâ€'))


def update_spk_weights(files, weights):
    if not files:
        return "1.0"

    weights = re.split(r'[ï¼š:]\s*', weights)
    weights = [weight for weight in weights if weight]

    f_len = len(files)
    w_len = len(weights)
    if f_len <= w_len:
        new_weights = weights[:f_len]
    else:
        new_weights = weights + ["1.0"]*(f_len-w_len)

    return ": ".join(new_weights)


ignore_transcribe = False
def audio_transcriber(audio_file):
    global ignore_transcribe

    if ignore_transcribe:
        ignore_transcribe = False
        audio_file = None

    if not audio_file is None and not asr is None:
        results = asr.transcribe(audio_file)
        text = results[0].text

        return text
    
    return gr.update()


def parse_tagged_text(text):
    parts = re.split(r'(<(?!(?:break))[^>]+>.*?</[^>]+>)', text)

    cut_texts = []
    tags = []
    for part in parts:
        if not part: continue

        match = re.search(r'<([^>]+)>(.*?)</[^>]+>', part)
        if match:
            tag_name = match.group(1)
            content = match.group(2)
            sub_parts = re.split(r'(<break:.*?>)', content)
            sub_parts = [p for p in sub_parts if p]
            tags.extend([tag_name]*len(sub_parts))
        else:    
            sub_parts = re.split(r'(<break:.*?>)', part)
            sub_parts = [p for p in sub_parts if p]
            tags.extend([None]*len(sub_parts))

        cut_texts.extend(sub_parts)
    
    for i in range(len(cut_texts)-1, -1, -1):
        if len(re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', '', cut_texts[i])) == 0:
            cut_texts.pop(i)
            tags.pop(i)

    return cut_texts, tags

def parse_speaker_weights(multi_spk_files, spk_weights):
    spk_weights = re.split(r'[ï¼š:]\s*', spk_weights)
    spk_audio = {multi_spk_files[i]: float(item) for i, item in enumerate(spk_weights) if item}
    return spk_audio


# é¢„è®¾å­˜å‚¨
presets = {}
def save_preset(name, prompt_audio, prompt_text, multi_spk_files, spk_weights):
    if not name:
        return gr.update(choices=list(presets.keys())), "è¯·è¾“å…¥é¢„è®¾åç§°"
    presets[name] = {
        "prompt_audio": prompt_audio,
        "prompt_text": prompt_text,
        "multi_spk_files": multi_spk_files,
        "spk_weights": spk_weights
    }
    return gr.update(choices=list(presets.keys()), value=name), f"é¢„è®¾ '{name}' å·²ä¿å­˜"

def load_preset(name):
    global ignore_transcribe
    ignore_transcribe = True

    if name not in presets:
        return None, "", None, "1.0"
    p = presets[name]
    return p["prompt_audio"], p["prompt_text"], p["multi_spk_files"], p["spk_weights"]


def vc_request(
    multi_spk_files, spk_weights,
    prompt_audio, prompt_text,
):
    try:
        start_time = time.time()

        audio = tts.infer_vc(
            spk_audio_path=parse_speaker_weights(multi_spk_files, spk_weights),
            prompt_audio_path=prompt_audio,
            prompt_audio_text=prompt_text,
        )

        end_time = time.time()

        infer_duration = end_time - start_time

        msg = (
            f"æˆåŠŸï¼\n"
            f"éŸ³é¢‘æ—¶é•¿: {audio.audio_len_s:.2f}s | "
            f"æ¨ç†è€—æ—¶: {infer_duration:.2f}s"
        )

        return (audio.samplerate, audio.audio_data), msg

    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, f"å¼‚å¸¸: {str(e)}"

def tts_request(
    multi_spk_files, spk_weights,
    prompt_audio, prompt_text,
    text,
    top_k, top_p, temperature, rep_penalty, noise_scale, speed,
    enable_enhance,
    is_cut_text, cut_punds, cut_minlen, cut_mute, cut_mute_scale_map,
    sovits_batch_size,
):
    try:
        start_time = time.time()

        spk_audio = parse_speaker_weights(multi_spk_files, spk_weights)

        cut_punds = set(cut_punds)
        cut_mute_scale_map = json.loads(cut_mute_scale_map)

        cut_texts, tags = parse_tagged_text(text)

        orig_idx = []
        spk_audio_paths = []
        prompt_audio_paths = []
        prompt_audio_texts = []
        texts = []

        for i in range(len(cut_texts)):
            result = re.search(r'<break:(.*?)/>', cut_texts[i])
            if result:
                cut_texts[i] = float(result.group(1))
                tags[i] = 'break'
            else:
                orig_idx.append(i)

                if tags[i] is None or tags[i] not in presets:
                    spk_audio_paths.append(spk_audio)
                    prompt_audio_paths.append(prompt_audio)
                    prompt_audio_texts.append(prompt_text)
                else:
                    p = presets[tags[i]]
                    spk_audio_paths.append(parse_speaker_weights(p["multi_spk_files"], p["spk_weights"]))
                    prompt_audio_paths.append(p["prompt_audio"])
                    prompt_audio_texts.append(p["prompt_text"])
                
                texts.append(cut_texts[i])
                    
        audios = tts.infer_batched(
            spk_audio_paths=spk_audio_paths,
            prompt_audio_paths=prompt_audio_paths,
            prompt_audio_texts=prompt_audio_texts,
            texts=texts,
            is_cut_text=is_cut_text,
            cut_punds=cut_punds,
            cut_minlen=cut_minlen,
            cut_mute=cut_mute,
            cut_mute_scale_map=cut_mute_scale_map,
            top_k=top_k,
            top_p=top_p,
            temperature=temperature,
            repetition_penalty=rep_penalty,
            noise_scale=noise_scale,
            speed=speed,
            sovits_batch_size=sovits_batch_size,
        )

        samplerate = audios[0].samplerate

        audio_data = []
        audio_len_s = 0
        for i in range(len(cut_texts)):
            if tags[i] == "break":
                audio_data.append(np.zeros((int(cut_texts[i] * samplerate),)))
                audio_len_s += cut_texts[i]
            else:
                tmp_audio = audios[orig_idx.index(i)]
                audio_data.append(tmp_audio.audio_data)
                audio_len_s += tmp_audio.audio_len_s
        
        audio_data = np.concatenate(audio_data)
        
        audio = AudioClip(None, audio_data, samplerate, audio_len_s, None, None)
        
        end_time = time.time()
        
        if enable_enhance:
            audio.audio_data = enhance_audio(audio.audio_data, audio.samplerate)

        infer_duration = end_time - start_time
        rtf = infer_duration / audio.audio_len_s

        msg = (
            f"æˆåŠŸï¼\n"
            f"éŸ³é¢‘æ—¶é•¿: {audio.audio_len_s:.2f}s | "
            f"æ¨ç†è€—æ—¶: {infer_duration:.2f}s | "
            f"RTF: {rtf:.3f}"
        )

        filename = f"result_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:4]}.wav"
        save_path = os.path.join(HISTORY_DIR, filename)
        audio.save(save_path)
        history_entry = [datetime.now().strftime("%H:%M:%S"), text[:20] + "...", save_path]

        return (audio.samplerate, audio.audio_data), msg, history_entry
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, f"å¼‚å¸¸: {str(e)}", None


# --- UI ç•Œé¢ ---
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# GSV-TTS")

    with gr.Tabs():
        with gr.TabItem("æ–‡æœ¬è½¬è¯­éŸ³ (TTS)"):

            history_state = gr.State([])
            
            with gr.Group():
                gr.Markdown("### ç¬¬ä¸€æ­¥ï¼šåŠ è½½æ¨¡å‹æ–‡ä»¶")
                with gr.Row():
                    gpt_path = gr.Textbox(label="1. GPT æ¨¡å‹è·¯å¾„ (.ckpt)", placeholder="ç•™ç©ºåˆ™é»˜è®¤æ¨¡å‹", scale=1)
                    sovits_path = gr.Textbox(label="2. SoVITS æ¨¡å‹è·¯å¾„ (.pth)", placeholder="ç•™ç©ºåˆ™é»˜è®¤æ¨¡å‹", scale=1)

            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### ç¬¬äºŒæ­¥ï¼šåˆæˆå†…å®¹ï¼ˆæ”¯æŒå¤šè¯´è¯äººï¼Œæ”¯æŒåœé¡¿æ ‡ç­¾ï¼‰")
                    text = gr.Textbox(label="åˆæˆç›®æ ‡æ–‡æœ¬", lines=5, value="è°ç½•è§?å•Šï¼Ÿéª‚è°ç½•è§ï¼")
                    enable_enhance = gr.Checkbox(label="å¯ç”¨éŸ³é¢‘å¢å¼º", value=True)
                    
                    with gr.Accordion("ç”Ÿæˆå‚æ•°", open=False):
                        speed = gr.Slider(0.5, 2.0, 1.0, step=0.1, label="è¯­é€Ÿ")
                        noise_scale = gr.Slider(0.1, 1.0, 0.5, step=0.05, label="å™ªå£°æ¯”ä¾‹")
                        temperature = gr.Slider(0.1, 1.5, 1.0, label="æ¸©åº¦")
                        top_k = gr.Slider(1, 50, 15, step=1, label="Top K")
                        top_p = gr.Slider(0.1, 1.0, 1.0, label="Top P")
                        rep_penalty = gr.Slider(1.0, 2.0, 1.35, label="é‡å¤æƒ©ç½š")
                        sovits_batch_size = gr.Number(label="SoVITSæœ€å¤§å¹¶è¡Œæ¨ç†å¤§å°", value=10)
                        is_cut_text = gr.Checkbox(label="æ˜¯å¦åˆ‡åˆ†æ–‡æœ¬", value=True)
                        cut_punds = gr.Textbox(label="åˆ‡åˆ†æ ‡ç‚¹", value='{"ã€‚", ".", "?", "ï¼Ÿ", "!", "ï¼", ",", "ï¼Œ", ":", "ï¼š", ";", "ï¼›", "ã€"}')
                        cut_minlen = gr.Number(label="æœ€å°åˆ‡åˆ†é•¿åº¦", value=10)
                        cut_mute = gr.Number(label="åˆ‡åˆ†é™éŸ³æ—¶é•¿(s)", value=0.2)
                        cut_mute_scale_map = gr.Textbox(label="æ ‡ç‚¹é™éŸ³ç¼©æ”¾æ˜ å°„", value='{".": 1.5, "ã€‚": 1.5, "?": 1.5, "ï¼Ÿ": 1.5, "!": 1.5, "ï¼": 1.5, ",": 0.8, "ï¼Œ": 0.8, "ã€": 0.6}')

                with gr.Column(scale=1):
                    gr.Markdown("### ç¬¬ä¸‰æ­¥ï¼šé£æ ¼ä¸éŸ³è‰²å‚è€ƒ")
                    
                    with gr.Row():
                        preset_dropdown = gr.Dropdown(choices=[], label="åŠ è½½é¢„è®¾", scale=2)
                        preset_name = gr.Textbox(label="é¢„è®¾åç§°", placeholder="ä¿å­˜å½“å‰è®¾ç½®ä¸º...", scale=2)
                        save_btn = gr.Button("ğŸ’¾ ä¿å­˜é¢„è®¾", scale=1)

                    with gr.Tab("é£æ ¼å‚è€ƒ"):
                        prompt_audio = gr.Audio(label="é£æ ¼å‚è€ƒéŸ³é¢‘ (å†³å®šè¯­æ°”ã€æƒ…æ„Ÿ)", type="filepath")
                        prompt_text = gr.Textbox(label="é£æ ¼å‚è€ƒéŸ³é¢‘å¯¹åº”æ–‡æœ¬", placeholder="è¾“å…¥å‚è€ƒéŸ³é¢‘ä¸­çš„æ–‡æœ¬å†…å®¹")

                    with gr.Tab("éŸ³è‰²å‚è€ƒï¼ˆæ”¯æŒå¤šéŸ³è‰²èåˆï¼‰"):
                        multi_spk_files = gr.File(label="å¯ä¸Šä¼ å¤šä¸ªéŸ³è‰²å‚è€ƒéŸ³é¢‘", file_count="multiple")
                        spk_weights = gr.Textbox(label="éŸ³è‰²æƒé‡ (ç”¨å†’å·åˆ†éš”)", value="1.0", placeholder="ä¾‹å¦‚: 1.0: 1.0")

            with gr.Group():
                btn = gr.Button("ğŸ”¥ å¼€å§‹è¯­éŸ³åˆæˆ", variant="primary", size="lg")
                with gr.Row():
                    with gr.Column(scale=2):
                        output_audio = gr.Audio(label="ç”Ÿæˆçš„éŸ³é¢‘ç»“æœ")
                        log_output = gr.Textbox(label="ç³»ç»ŸçŠ¶æ€ä¿¡æ¯")
                    
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ•’ æœ€è¿‘ç”Ÿæˆå†å²")
                        history_display = gr.Dataset(
                            components=[gr.Textbox(visible=False)],
                            label="ç‚¹å‡»ä¸‹æ–¹æ¡ç›®å¯é‡æ–°åŠ è½½éŸ³é¢‘",
                            samples=[],
                            type="values"
                        )


        with gr.TabItem("éŸ³è‰²è¿ç§» (VC)"):
            gr.Markdown("### å°†ä¸€æ®µéŸ³é¢‘çš„å†…å®¹è¿ç§»åˆ°å¦ä¸€ä¸ªäººçš„éŸ³è‰²ä¸Š")
            
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("#### 1. æºéŸ³é¢‘å‚è€ƒ")
                    vc_source_audio = gr.Audio(label="ä¸Šä¼ æºéŸ³é¢‘", type="filepath")
                    vc_source_text = gr.Textbox(label="æºéŸ³é¢‘å¯¹åº”æ–‡æœ¬", placeholder="è¾“å…¥æºéŸ³é¢‘ä¸­çš„æ–‡æœ¬å†…å®¹", lines=2)
                    
                    gr.Markdown("#### 2. ç›®æ ‡éŸ³è‰²å‚è€ƒï¼ˆæ”¯æŒå¤šéŸ³è‰²èåˆï¼‰")
                    vc_multi_spk_files = gr.File(label="å¯ä¸Šä¼ å¤šä¸ªéŸ³è‰²å‚è€ƒéŸ³é¢‘", file_count="multiple")
                    vc_spk_weights = gr.Textbox(label="éŸ³è‰²æƒé‡ (ç”¨å†’å·åˆ†éš”)", value="1.0", placeholder="ä¾‹å¦‚: 1.0: 1.0")
                
                with gr.Column(scale=1):
                    gr.Markdown("#### 3. æ‰§è¡Œä¸è¾“å‡º")
                    vc_btn = gr.Button("ğŸš€ å¼€å§‹éŸ³è‰²è¿ç§»", variant="primary", size="lg")
                    
                    vc_output_audio = gr.Audio(label="éŸ³è‰²è¿ç§»ç»“æœ", interactive=False)
                    vc_log_output = gr.Textbox(label="å¤„ç†æ—¥å¿—", lines=5)


    def update_history(history_entry, current_history):
        if history_entry is None:
            return current_history, gr.update(samples=current_history)
        
        current_history.insert(0, history_entry)
        current_history = current_history[:10]
        
        return current_history, gr.update(samples=current_history)

    def load_from_history(selected_row_data):
        if selected_row_data and len(selected_row_data) > 0:
            audio_path = selected_row_data[-1] 
            return audio_path
        return None

    save_btn.click(
        fn=save_preset,
        inputs=[preset_name, prompt_audio, prompt_text, multi_spk_files, spk_weights],
        outputs=[preset_dropdown, log_output]
    )
    
    preset_dropdown.change(
        fn=load_preset,
        inputs=[preset_dropdown],
        outputs=[prompt_audio, prompt_text, multi_spk_files, spk_weights]
    )

    multi_spk_files.change(
        fn=update_spk_weights,
        inputs=[multi_spk_files, spk_weights],
        outputs=spk_weights
    )

    vc_multi_spk_files.change(
        fn=update_spk_weights,
        inputs=[vc_multi_spk_files, vc_spk_weights],
        outputs=vc_spk_weights
    )

    prompt_audio.change(
        fn=audio_transcriber,
        inputs=prompt_audio,
        outputs=prompt_text
    )

    vc_source_audio.change(
        fn=audio_transcriber,
        inputs=vc_source_audio,
        outputs=vc_source_text
    )

    gpt_path.change(
        fn=upload_gpt,
        inputs=gpt_path
    )

    sovits_path.change(
        fn=upload_sovits,
        inputs=sovits_path
    )

    temp_history_entry = gr.State()

    btn.click(
        fn=tts_request,
        inputs=[
            multi_spk_files, spk_weights,
            prompt_audio, prompt_text,
            text,
            top_k, top_p, temperature, rep_penalty, noise_scale, speed,
            enable_enhance,
            is_cut_text, cut_punds, cut_minlen, cut_mute, cut_mute_scale_map,
            sovits_batch_size,
        ],
        outputs=[output_audio, log_output, temp_history_entry]
    ).then(
        fn=update_history,
        inputs=[temp_history_entry, history_state],
        outputs=[history_state, history_display]
    )

    vc_btn.click(
        fn=vc_request,
        inputs=[
            vc_multi_spk_files, vc_spk_weights,
            vc_source_audio, vc_source_text,
        ],
        outputs=[vc_output_audio, vc_log_output]
    )

    history_display.click(
        fn=load_from_history,
        inputs=[history_display],
        outputs=[output_audio]
    )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="GSV-TTS")
    parser.add_argument("--gpt_cache_len", type=int, default=512, help="GPT KV cache ä¸Šä¸‹æ–‡é•¿åº¦")
    parser.add_argument("--gpt_batch_size", type=int, default=8, help="GPT æœ€å¤§å¹¶è¡Œæ¨ç†å¤§å°")
    parser.add_argument("--use_bert", type=bool, default=True, help="ä½¿ç”¨BERTæå‡ä¸­æ–‡è¯­ä¹‰ç†è§£èƒ½åŠ›")
    parser.add_argument("--use_flash_attn", type=bool, default=False, help="ä½¿ç”¨Flash AttnåŠ é€Ÿæ¨ç†")
    parser.add_argument("--use_asr", type=bool, default=True, help="ä½¿ç”¨ASRè‡ªåŠ¨è¯†åˆ«éŸ³é¢‘æ–‡æœ¬")
    parser.add_argument("--port", type=int, default=9881, help="Gradio ç«¯å£å·")
    parser.add_argument("--share", action="store_true", help="æ˜¯å¦å¼€å¯å…¬ç½‘åˆ†äº«")
    
    args, _ = parser.parse_known_args()

    HISTORY_DIR = "output_history"
    os.makedirs(HISTORY_DIR, exist_ok=True)

    if args.use_asr:
        from qwen_asr import Qwen3ASRModel

        asr = Qwen3ASRModel.from_pretrained(
            "models\qwen3_asr",
            dtype=torch.bfloat16,
            device_map="cuda:0",
            attn_implementation="flash_attention_2" if args.use_flash_attn else None,
        )
    else:
        asr = None

    tts = TTS(
        gpt_cache=[(1, args.gpt_cache_len)] + [(B, args.gpt_cache_len) for B in range(4, args.gpt_batch_size-1, 4)] + [(args.gpt_batch_size, args.gpt_cache_len)],
        sovits_cache=[],
        use_bert=args.use_bert,
        use_flash_attn=args.use_flash_attn,
        models_dir="models",
    )
    
    demo.launch(
        server_port=args.port,
        share=args.share,
        inbrowser=True
    )